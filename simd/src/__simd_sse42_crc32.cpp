/* Generated by https://github.com/corsix/fast-crc32/ using: */
/* E:\Documents\programming\crc32\fast-crc32\generate.exe -i sse -p crc32c -a v2s2 */
/* MIT licensed */

#include <acul/api.hpp>
#include <nmmintrin.h>
#include <stddef.h>
#include <stdint.h>
#include <wmmintrin.h>

#if defined(_MSC_VER)
    #define CRC_AINLINE  static __forceinline
    #define CRC_ALIGN(n) __declspec(align(n))
#else
    #define CRC_AINLINE  static __inline __attribute__((always_inline))
    #define CRC_ALIGN(n) __attribute__((aligned(n)))
#endif

#define clmul_lo(a, b) (_mm_clmulepi64_si128((a), (b), 0))
#define clmul_hi(a, b) (_mm_clmulepi64_si128((a), (b), 17))

CRC_AINLINE __m128i clmul_scalar(uint32_t a, uint32_t b)
{
    return _mm_clmulepi64_si128(_mm_cvtsi32_si128(a), _mm_cvtsi32_si128(b), 0);
}

static uint32_t xnmodp(uint64_t n) /* x^n mod P, in log(n) time */
{
    uint64_t stack = ~(uint64_t)1;
    uint32_t acc, low;
    for (; n > 191; n = (n >> 1) - 16) { stack = (stack << 1) + (n & 1); }
    stack = ~stack;
    acc = ((uint32_t)0x80000000) >> (n & 31);
    for (n >>= 5; n; --n) { acc = _mm_crc32_u32(acc, 0); }
    while ((low = stack & 1), stack >>= 1)
    {
        __m128i x = _mm_cvtsi32_si128(acc);
        uint64_t y = _mm_cvtsi128_si64(_mm_clmulepi64_si128(x, x, 0));
        acc = _mm_crc32_u64(0, y << low);
    }
    return acc;
}

CRC_AINLINE __m128i crc_shift(uint32_t crc, size_t nbytes) { return clmul_scalar(crc, xnmodp(nbytes * 8 - 33)); }

extern "C" APPLIB_API uint32_t acul_crc32(uint32_t crc0, const char *buf, size_t len)
{
    crc0 = ~crc0;
    for (; len && ((uintptr_t)buf & 7); --len) crc0 = _mm_crc32_u8(crc0, *buf++);
    if (((uintptr_t)buf & 8) && len >= 8)
    {
        crc0 = _mm_crc32_u64(crc0, *(const uint64_t *)buf);
        buf += 8;
        len -= 8;
    }
    if (len >= 48)
    {
        size_t blk = (len - 0) / 48;
        size_t klen = blk * 8;
        const char *buf2 = buf + klen * 2;
        uint32_t crc1 = 0;
        __m128i vc0;
        __m128i vc1;
        uint64_t vc;
        /* First vector chunk. */
        __m128i x0 = _mm_loadu_si128((const __m128i *)buf2), y0;
        __m128i x1 = _mm_loadu_si128((const __m128i *)(buf2 + 16)), y1;
        __m128i k;
        k = _mm_setr_epi32(0x3da6d0cb, 0, 0xba4fc28e, 0);
        buf2 += 32;
        len -= 48;
        /* Main loop. */
        while (len >= 48)
        {
            y0 = clmul_lo(x0, k), x0 = clmul_hi(x0, k);
            y1 = clmul_lo(x1, k), x1 = clmul_hi(x1, k);
            y0 = _mm_xor_si128(y0, _mm_loadu_si128((const __m128i *)buf2)), x0 = _mm_xor_si128(x0, y0);
            y1 = _mm_xor_si128(y1, _mm_loadu_si128((const __m128i *)(buf2 + 16))), x1 = _mm_xor_si128(x1, y1);
            crc0 = _mm_crc32_u64(crc0, *(const uint64_t *)buf);
            crc1 = _mm_crc32_u64(crc1, *(const uint64_t *)(buf + klen));
            buf += 8;
            buf2 += 32;
            len -= 48;
        }
        /* Reduce x0 ... x1 to just x0. */
        k = _mm_setr_epi32(0xf20c0dfe, 0, 0x493c7d27, 0);
        y0 = clmul_lo(x0, k), x0 = clmul_hi(x0, k);
        y0 = _mm_xor_si128(y0, x1), x0 = _mm_xor_si128(x0, y0);
        /* Final scalar chunk. */
        crc0 = _mm_crc32_u64(crc0, *(const uint64_t *)buf);
        crc1 = _mm_crc32_u64(crc1, *(const uint64_t *)(buf + klen));
        vc0 = crc_shift(crc0, klen + blk * 32);
        vc1 = crc_shift(crc1, 0 + blk * 32);
        vc = _mm_extract_epi64(_mm_xor_si128(vc0, vc1), 0);
        /* Reduce 128 bits to 32 bits, and multiply by x^32. */
        crc0 = _mm_crc32_u64(0, _mm_extract_epi64(x0, 0));
        crc0 = _mm_crc32_u64(crc0, vc ^ _mm_extract_epi64(x0, 1));
        buf = buf2;
    }
    for (; len >= 8; buf += 8, len -= 8) crc0 = _mm_crc32_u64(crc0, *(const uint64_t *)buf);
    for (; len; --len) crc0 = _mm_crc32_u8(crc0, *buf++);
    return ~crc0;
}